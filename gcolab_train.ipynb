{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03IVwgTW6vBw"
      },
      "source": [
        "## Ноутбук для запуска обучения в Colab 🧘\n",
        "### Подгрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMg7hmJc7ISJ"
      },
      "source": [
        "Клонирование целевого репозитория"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbumD8vfta8a",
        "outputId": "d0f935c0-53b1-4a56-ea81-d4700c623950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT_NER'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 42 (delta 18), reused 31 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (42/42), 47.87 KiB | 2.82 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/graviada/BERT_NER.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Для очистки загруженных файлов\n",
        "# !rm -rf /content/BERT_NER/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxFEmgYQ7Mii"
      },
      "source": [
        "Загрузка необходимых библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP6YdVbptp7j",
        "outputId": "cadf9ba9-9b0d-4944-b791-0fda4d63e64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers seqeval evaluate wandb --q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRWV1DzS7P71"
      },
      "source": [
        "---\n",
        "Раздел танцев с бубнами, поскольку в версиях PyTorch после 1.12 возникают проблемы по запуску Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb_Wjpk8zCGR",
        "outputId": "2f62a20d-0c39-4ab9-c2de-8d61e65bbda5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U --q\n",
        "\n",
        "!pip uninstall torch -y --q\n",
        "!pip uninstall torchaudio -y --q\n",
        "!pip uninstall torchdata -y --q\n",
        "!pip uninstall torchtext -y --q\n",
        "!pip uninstall torchvision -y --q\n",
        "!pip uninstall fastai -y --q\n",
        "\n",
        "!pip install torch==1.12.0 --q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxTAkjDu7dPA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgZ2BXjB7oAp"
      },
      "source": [
        "### Импорт парочки библиотек + проверка версии PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pacSU8wnuAyn",
        "outputId": "980b565d-12c4-44aa-b88f-eaf0f3a9ea7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.12.0+cu102\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "sys.path.append('/content/BERT_NER/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHkIfatH7voN"
      },
      "source": [
        "### Главная ячейка с запуском обучения ⚡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5sU5C6suh7u",
        "outputId": "514ffafc-4c48-4094-8280-d12844015814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-07 13:00:50.334408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 6.34k/6.34k [00:00<00:00, 3.96MB/s]\n",
            "Директория логгирования: /content/result/logging\n",
            "Downloading builder script: 100% 3.98k/3.98k [00:00<00:00, 3.95MB/s]\n",
            "Downloading readme: 100% 3.75k/3.75k [00:00<00:00, 2.69MB/s]\n",
            "Downloading and preparing dataset wikineural/ru to /root/.cache/huggingface/datasets/tner___wikineural/ru/1.0.0/b6ab60912b5d54b25b8541dcac17a8c8cfe3897fdff48bcdff0a4e3945f18c43...\n",
            "Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/7.90M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 16.4k/7.90M [00:00<01:40, 78.8kB/s]\u001b[A\n",
            "Downloading data:   1% 51.2k/7.90M [00:00<01:00, 130kB/s] \u001b[A\n",
            "Downloading data:   2% 135k/7.90M [00:00<00:30, 252kB/s] \u001b[A\n",
            "Downloading data:   4% 314k/7.90M [00:00<00:15, 488kB/s]\u001b[A\n",
            "Downloading data:   8% 656k/7.90M [00:01<00:08, 896kB/s]\u001b[A\n",
            "Downloading data:  17% 1.36M/7.90M [00:01<00:03, 1.72MB/s]\u001b[A\n",
            "Downloading data:  35% 2.75M/7.90M [00:01<00:01, 3.31MB/s]\u001b[A\n",
            "Downloading data: 100% 7.90M/7.90M [00:01<00:00, 4.49MB/s]\n",
            "Downloading data files:  33% 1/3 [00:03<00:07,  3.51s/it]\n",
            "Downloading data:   0% 0.00/67.2M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 19.5k/67.2M [00:00<11:59, 93.4kB/s]\u001b[A\n",
            "Downloading data:   0% 52.2k/67.2M [00:00<08:34, 131kB/s] \u001b[A\n",
            "Downloading data:   0% 139k/67.2M [00:00<04:17, 261kB/s] \u001b[A\n",
            "Downloading data:   0% 313k/67.2M [00:00<02:18, 484kB/s]\u001b[A\n",
            "Downloading data:   1% 662k/67.2M [00:01<01:14, 897kB/s]\u001b[A\n",
            "Downloading data:   2% 1.34M/67.2M [00:01<00:38, 1.70MB/s]\u001b[A\n",
            "Downloading data:   4% 2.71M/67.2M [00:01<00:19, 3.26MB/s]\u001b[A\n",
            "Downloading data:   8% 5.46M/67.2M [00:01<00:09, 6.40MB/s]\u001b[A\n",
            "Downloading data:  14% 9.40M/67.2M [00:01<00:05, 10.2MB/s]\u001b[A\n",
            "Downloading data:  20% 13.6M/67.2M [00:02<00:04, 13.1MB/s]\u001b[A\n",
            "Downloading data:  26% 17.6M/67.2M [00:02<00:03, 14.9MB/s]\u001b[A\n",
            "Downloading data:  32% 21.5M/67.2M [00:02<00:02, 15.9MB/s]\u001b[A\n",
            "Downloading data:  38% 25.6M/67.2M [00:02<00:02, 16.5MB/s]\u001b[A\n",
            "Downloading data:  44% 29.5M/67.2M [00:02<00:02, 17.4MB/s]\u001b[A\n",
            "Downloading data:  50% 33.6M/67.2M [00:03<00:01, 17.6MB/s]\u001b[A\n",
            "Downloading data:  56% 37.7M/67.2M [00:03<00:01, 18.0MB/s]\u001b[A\n",
            "Downloading data:  62% 41.7M/67.2M [00:03<00:01, 18.3MB/s]\u001b[A\n",
            "Downloading data:  68% 45.8M/67.2M [00:03<00:01, 18.5MB/s]\u001b[A\n",
            "Downloading data:  73% 48.7M/67.2M [00:04<00:01, 17.2MB/s]\u001b[A\n",
            "Downloading data:  79% 52.8M/67.2M [00:04<00:00, 17.7MB/s]\u001b[A\n",
            "Downloading data:  85% 56.8M/67.2M [00:04<00:00, 18.1MB/s]\u001b[A\n",
            "Downloading data:  91% 60.9M/67.2M [00:04<00:00, 18.3MB/s]\u001b[A\n",
            "Downloading data: 100% 67.2M/67.2M [00:04<00:00, 13.6MB/s]\n",
            "Downloading data files:  67% 2/3 [00:10<00:05,  5.79s/it]\n",
            "Downloading data:   0% 0.00/7.97M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 1.02k/7.97M [00:00<29:18, 4.53kB/s]\u001b[A\n",
            "Downloading data:   0% 33.8k/7.97M [00:00<01:30, 87.3kB/s]\u001b[A\n",
            "Downloading data:   1% 86.0k/7.97M [00:00<00:40, 195kB/s] \u001b[A\n",
            "Downloading data:   1% 112k/7.97M [00:00<00:37, 207kB/s] \u001b[A\n",
            "Downloading data:   3% 208k/7.97M [00:00<00:19, 390kB/s]\u001b[A\n",
            "Downloading data:   3% 252k/7.97M [00:00<00:19, 394kB/s]\u001b[A\n",
            "Downloading data:   5% 434k/7.97M [00:01<00:09, 754kB/s]\u001b[A\n",
            "Downloading data:   6% 515k/7.97M [00:01<00:10, 745kB/s]\u001b[A\n",
            "Downloading data:  11% 887k/7.97M [00:01<00:04, 1.50MB/s]\u001b[A\n",
            "Downloading data:  13% 1.04M/7.97M [00:01<00:04, 1.46MB/s]\u001b[A\n",
            "Downloading data:  22% 1.79M/7.97M [00:01<00:02, 3.00MB/s]\u001b[A\n",
            "Downloading data:  26% 2.10M/7.97M [00:01<00:02, 2.85MB/s]\u001b[A\n",
            "Downloading data:  45% 3.61M/7.97M [00:01<00:00, 6.05MB/s]\u001b[A\n",
            "Downloading data:  53% 4.24M/7.97M [00:01<00:00, 5.87MB/s]\u001b[A\n",
            "Downloading data:  86% 6.84M/7.97M [00:01<00:00, 10.7MB/s]\u001b[A\n",
            "Downloading data: 100% 7.97M/7.97M [00:02<00:00, 3.87MB/s]\n",
            "Downloading data files: 100% 3/3 [00:15<00:00,  5.00s/it]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1308.95it/s]\n",
            "Dataset wikineural downloaded and prepared to /root/.cache/huggingface/datasets/tner___wikineural/ru/1.0.0/b6ab60912b5d54b25b8541dcac17a8c8cfe3897fdff48bcdff0a4e3945f18c43. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 422.36it/s]\n",
            "Инициализирован config: TnerWikiNeural(datadict=DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tokens', 'tags'],\n",
            "        num_rows: 27696\n",
            "    })\n",
            "    valid: Dataset({\n",
            "        features: ['tokens', 'tags'],\n",
            "        num_rows: 3462\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tokens', 'tags'],\n",
            "        num_rows: 3474\n",
            "    })\n",
            "}))\n",
            "Downloading (…)okenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 290kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 521k/521k [00:00<00:00, 1.24MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 494kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 806/806 [00:00<00:00, 3.89MB/s]\n",
            "Данные обработаны токенизатором\n",
            "Downloading pytorch_model.bin: 100% 516M/516M [00:03<00:00, 139MB/s]\n",
            "Some weights of the model checkpoint at cointegrated/LaBSE-en-ru were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/LaBSE-en-ru and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Модель labse загружена\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230607_130209-g7hksxvc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/result\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/graviada/BERTmodels_NER\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/graviada/BERTmodels_NER/runs/g7hksxvc\u001b[0m\n",
            "  0% 0/3464 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{'loss': 3.8404, 'learning_rate': 0.0, 'epoch': 0.0}\n",
            "  3% 114/3464 [00:16<07:55,  7.04it/s]"
          ]
        }
      ],
      "source": [
        "!python /content/BERT_NER/main.py \\\n",
        " --data-name wikineural \\\n",
        " --model-name labse \\\n",
        " --result-dir /content/result \\\n",
        " --num-epochs 4 \\\n",
        " --max-length 200 \\\n",
        " --batch-size 32 \\\n",
        " --learning-rate 2e-5 \\\n",
        " --dropout 0.1 \\\n",
        " --weight-decay 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rdss3ISjSDz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
